# -*- coding: utf-8 -*-
"""Chatbot_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-2zjfauZmYp8mzr9_-EAURyADc771-5W

#ChatBot_Project
"""

import nltk
import json
import numpy as np
import random
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import SGD
from sklearn.metrics import precision_score
from sklearn.model_selection import train_test_split

"""punkt is a pre-trained model included in the Natural Language Toolkit (nltk) that is used for tokenization. Tokenization is the process of splitting text into individual units, such as words or sentences"""

nltk.download('punkt')

"""wordnet is used to download the WordNet corpus, which is necessary for performing lemmatization with NLTK's"""

nltk.download('wordnet')

"""# Data preprocessing"""

words = []
classes = []
documents = []
ignore_letters = ['?', '!', '.', ',']
lemmatizer = WordNetLemmatizer()

data_file = open('intents.json').read()
intents = json.loads(data_file)

for intent in intents['intents']:
    for pattern in intent['patterns']:
        word_list = nltk.word_tokenize(pattern)
        words.extend(word_list)
        documents.append((word_list, intent['tag']))
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]
words = sorted(list(set(words)))
classes = sorted(list(set(classes)))

print(len(documents), "documents")
print(len(classes), "classes", classes)
print(len(words), "unique lemmatized words", words)

"""#Training Data"""

training = []
empty_array = [0] * len(classes)

for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
    output_row = list(empty_array)
    output_row[classes.index(document[1])] = 1
    training.append([bag, output_row])

random.shuffle(training)
trainings = np.array(training, dtype=object)

# Split data into training and testing sets
train_x, test_x, train_y, test_y = train_test_split(list(trainings[:, 0]), list(trainings[:, 1]), test_size=0.2, random_state=42)

"""#Model creating"""

# Build the model
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Compile the model
sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# Train the model
hist = model.fit(np.array(train_x), np.array(train_y), epochs=150, batch_size=5, verbose=1)

"""#END"""